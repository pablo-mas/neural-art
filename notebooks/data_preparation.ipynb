{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1de8a528",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bac68fee-f859-4d79-a020-ecd6c6cfc4dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:10:44.529131Z",
     "start_time": "2021-09-02T09:10:44.509761Z"
    }
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0c2feac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T09:36:20.820460Z",
     "start_time": "2021-09-02T09:36:20.796364Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "from neuralart.data import get_chan_data, create_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44e5a94d",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Download the Wikiart dataset from Chan's repository"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77a8b5e-5a2c-4aa5-804a-62640fdac60d",
   "metadata": {
    "tags": []
   },
   "source": [
    "```bash\n",
    "wget http://web.fsktm.um.edu.my/~cschan/source/ICIP2017/wikiart.zip\n",
    "````\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d004d98-4b9f-4a02-8bf0-6f44ecc6e744",
   "metadata": {},
   "source": [
    "```bash\n",
    "wget http://web.fsktm.um.edu.my/~cschan/source/ICIP2017/wikiart_csv.zip\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f37ab18a-d2c4-4e6c-980d-bf2c644db020",
   "metadata": {},
   "source": [
    "# Parse Chan's dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642dc50c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-23T20:51:26.827074Z",
     "start_time": "2021-08-23T20:51:13.824048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(78748, 10)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Path to the folder containing Chan's csv files (ie. wikiart_csv)\n",
    "chan_csv_folder_path = \"../raw_data/wikiart/csv_chan\"\n",
    "# Path to the folder containing Chan's images from wikiart (ie. wikiart)\n",
    "chan_image_folder_path = \"../raw_data/wikiart/dataset_chan\"\n",
    "# Path to save a new csv file containing all the information about Chan's dataset\n",
    "csv_output_path = \"../raw_data/wikiart\"\n",
    "\n",
    "chan_data = get_chan_data(chan_csv_folder_path, \n",
    "                          chan_image_folder_path, \n",
    "                          csv_output_path=csv_output_path)\n",
    "chan_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644c923f-cc44-45e6-ab06-0e1bfc099f1d",
   "metadata": {},
   "source": [
    "# Create a small dataset with 100 paintings per art style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f046fcd-6f1f-422a-8118-69d1ce959785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 images copied\n",
      "Done: 2600 image(s) copied\n"
     ]
    }
   ],
   "source": [
    "# Path to save a new csv file containing all the information about the new dataset\n",
    "csv_file_name =  \"../raw_data/wikiart/wikiart-target_style-class_27.csv\"\n",
    "# Path to create a new directory containing all the wikiart images used in the new dataset\n",
    "image_folder_output_path = \"../raw_data/wikiart\"\n",
    "\n",
    "# Image folder structure\n",
    "# Use flat=False if you plan to use the tf.keras.preprocessing.image_dataset_from_directory() function or \n",
    "# the create_dataset_from_directory() function from the trainer module to create the TensorFlow dataset\n",
    "flat=False\n",
    "# Use flat=True if you plan to use the tf.data.Dataset.from_tensor_slices() function or \n",
    "# the create_dataset_from_csv() function from the trainer module to create the TensorFlow dataset\n",
    "#flat=True \n",
    "\n",
    "# Train, Val, and test ratio to split the new dataset\n",
    "val_ratio=0.1\n",
    "test_ratio=0.1\n",
    "\n",
    "# Sample Chan's Dataset\n",
    "n=100\n",
    "strategy='drop'\n",
    "\n",
    "data=create_dataset(csv_file_name, \n",
    "                    random_state=123,\n",
    "                    n=n,\n",
    "                    strategy=strategy,\n",
    "                    chan_image_folder_path=chan_image_folder_path, \n",
    "                    csv_output_path=csv_output_path, \n",
    "                    image_folder_output_path=image_folder_output_path,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ab8360-354d-49cf-98f1-9570a9ab7d60",
   "metadata": {},
   "source": [
    "# Replicate our custom dataset of 8 art styles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c829496e-3476-4c34-9629-2647b6c3094e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500 images copied\n",
      "5000 images copied\n",
      "7500 images copied\n",
      "10000 images copied\n",
      "12500 images copied\n",
      "15000 images copied\n",
      "17500 images copied\n",
      "20000 images copied\n",
      "22500 images copied\n",
      "25000 images copied\n",
      "27500 images copied\n",
      "30000 images copied\n",
      "32500 images copied\n",
      "35000 images copied\n",
      "37500 images copied\n",
      "40000 images copied\n",
      "Done: 41268 image(s) copied\n"
     ]
    }
   ],
   "source": [
    "# Dictionary used to merge or drop some classes\n",
    "merge={'name': 'style_m1',\n",
    "       'merging':{'abstract_expressionism': 'abstract', 'action_painting': 'abstract', \n",
    "                  'analytical_cubism': 'cubism', 'art_nouveau_modern': None, 'baroque': None, \n",
    "                  'color_field_painting': 'color_field_painting', 'contemporary_realism': None, \n",
    "                  'cubism': 'cubism', 'early_renaissance': 'renaissance', \n",
    "                  'expressionism': 'expressionism', 'fauvism': None, 'high_renaissance': 'renaissance', \n",
    "                  'impressionism': 'impressionism', 'mannerism_late_renaissance': None, \n",
    "                  'minimalism': None, 'naive_art_primitivism': None, 'new_realism': None, \n",
    "                  'northern_renaissance': 'renaissance', 'pointillism': None, 'pop_art': None,\n",
    "                  'post_impressionism': None, 'realism': 'realism', 'rococo': None, \n",
    "                  'romanticism': 'romanticism', 'symbolism': None, 'synthetic_cubism': 'cubism', \n",
    "                  'ukiyo_e': None}}\n",
    "\n",
    "# Path to save a new csv file containing all the information about the new dataset\n",
    "csv_file_name =  \"../raw_data/wikiart/wikiart-target_style-class_27.csv\"\n",
    "# Path to create a new directory containing all the wikiart images used in the new dataset\n",
    "image_folder_output_path = \"../raw_data/wikiart\"\n",
    "\n",
    "# Image folder structure\n",
    "# Use flat=False if you plan to use the tf.keras.preprocessing.image_dataset_from_directory() function or \n",
    "# the create_dataset_from_directory() function from the trainer module to create the TensorFlow dataset\n",
    "flat=False\n",
    "# Use flat=True if you plan to use the tf.data.Dataset.from_tensor_slices() function or \n",
    "# the create_dataset_from_csv() function from the trainer module to create the TensorFlow dataset\n",
    "#flat=True \n",
    "\n",
    "# Train, Val, and test ratio to split the new dataset\n",
    "val_ratio=0.1\n",
    "test_ratio=0.1\n",
    "\n",
    "data=create_dataset(csv_file_name, \n",
    "                    merge=merge, \n",
    "                    random_state=123,\n",
    "                    chan_image_folder_path=chan_image_folder_path, \n",
    "                    csv_output_path=csv_output_path, \n",
    "                    image_folder_output_path=image_folder_output_path,\n",
    "                    val_ratio=val_ratio,\n",
    "                    test_ratio=test_ratio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
