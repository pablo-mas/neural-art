{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65201043",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Image classification with 'image_dataset_from_directory' method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2acab56",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Creating train/val/test datasets using split-folders package"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8e89473",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "The split-folders package allow us to create 3 folders of train/val/test images\n",
    "\n",
    "The input folder should have the following format (Gregoire's function output):\n",
    "\n",
    "input/\n",
    "\n",
    "    class1/\n",
    "        img1.jpg\n",
    "        img2.jpg\n",
    "        ...\n",
    "    class2/\n",
    "        imgWhatever.jpg\n",
    "        ...\n",
    "    ...\n",
    "    \n",
    "    \n",
    "And we get this\n",
    "\n",
    "output/\n",
    "\n",
    "    train/\n",
    "        class1/\n",
    "            img1.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imga.jpg\n",
    "            ...\n",
    "            \n",
    "    val/\n",
    "    \n",
    "        class1/\n",
    "        \n",
    "            img2.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imgb.jpg\n",
    "            ...\n",
    "    test/\n",
    "        class1/\n",
    "            img3.jpg\n",
    "            ...\n",
    "        class2/\n",
    "            imgc.jpg\n",
    "            ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1d47c979",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.8\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.1\n",
    "\n",
    "assert TRAIN_RATIO + VAL_RATIO + TEST_RATIO == 1\n",
    "assert TRAIN_RATIO != 0\n",
    "assert VAL_RATIO != 0\n",
    "assert TEST_RATIO != 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4440e985",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 11520 files [00:03, 2920.59 files/s]\n"
     ]
    }
   ],
   "source": [
    "import splitfolders\n",
    "!rm -rf \"/home/jupyter/wikiart/train_val_test_True_1440\"\n",
    "splitfolders.ratio(\"/home/jupyter/wikiart/wikiart-movement-genre_True-class_8-merge_mov-1-n_1440_max\", \n",
    "                   output=\"/home/jupyter/wikiart/train_val_test_True_1440\",\n",
    "                   seed=1337, ratio=(TRAIN_RATIO, VAL_RATIO, TEST_RATIO), \n",
    "                   group_prefix=None);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402189b",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "# Deep learning workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1339c8",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "747d2e1d",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import datetime\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, models, applications\n",
    "\n",
    "# Note : we are using TensorFlow Core v2.5.0, in TensorFlow Core v2.6.0 all the data \n",
    "# augmentation layers are part of tf.keras.layers\n",
    "from tensorflow.keras.layers.experimental.preprocessing import RandomFlip, RandomRotation, RandomZoom\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adamax"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3672d7bf",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8df9de21",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "USERNAME = 'pablo'\n",
    "MODEL = 'VGG16'\n",
    "\n",
    "MAIN_PATH = '/home/jupyter/' \n",
    "DATASETS_FOLDER = 'wikiart/train_val_test_True_1440/'\n",
    "\n",
    "TRAIN_DIR = MAIN_PATH + DATASETS_FOLDER + 'train'\n",
    "VAL_DIR = MAIN_PATH + DATASETS_FOLDER + 'val'\n",
    "TEST_DIR = MAIN_PATH + DATASETS_FOLDER + 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "515604fa",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128 # Hyper param, you can tune it\n",
    "EPOCHS = 1000 # Large number, early stopping to stop training before this number\n",
    "IMG_HEIGHT = 224 # VGG's dim\n",
    "IMG_WIDTH = 224 # VGG's dim\n",
    "NUM_CLASSES = 8 # Number of art styles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "645f2ba4",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Datasets setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715b27de",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9428e35a",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 9216 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "train_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=TRAIN_DIR,\n",
    "    labels='inferred',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    label_mode='categorical',\n",
    "    shuffle=True)\n",
    "\n",
    "assert len(train_ds.class_names) == NUM_CLASSES "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844a9cd",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13888d3e",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1152 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "val_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=VAL_DIR,\n",
    "    labels='inferred',\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    label_mode='categorical',\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "assert len(val_ds.class_names) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6977173f",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75a27e61",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1152 files belonging to 8 classes.\n"
     ]
    }
   ],
   "source": [
    "test_ds = tf.keras.preprocessing.image_dataset_from_directory(\n",
    "    directory=TEST_DIR,\n",
    "    labels='inferred', # labels are generated from the directory structure\n",
    "    image_size=(IMG_HEIGHT, IMG_WIDTH),\n",
    "    label_mode='categorical', # labels are encoded as a categorical vector (e.g. for categorical_crossentropy loss\n",
    "    batch_size=BATCH_SIZE)\n",
    "\n",
    "assert len(test_ds.class_names) == NUM_CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94d65692",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11520"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_images_count = (int(len(list(train_ds)))+int(len(list(val_ds)))+int(len(list(test_ds))))*BATCH_SIZE\n",
    "# total_images_count = 33011 + 4123 + 4134\n",
    "total_images_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12731cdd",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Dataset optimization for performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "819d1000",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "\n",
    "# Optimizing the dataset by caching and prefetching the data\n",
    "train_ds = train_ds.cache().shuffle(int(total_images_count)).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)\n",
    "test_ds = test_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e8869c",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "## Model : transfer learning with VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88db36e3",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### VGG16 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ec95926d",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "layer_model = applications.VGG16(\n",
    "    include_top=False, # We do not include VGG classification layers\n",
    "    weights='imagenet', # We import VGG pre-trained on ImageNet\n",
    "    input_shape=(IMG_HEIGHT, IMG_WIDTH, 3), \n",
    "    classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce94b741",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x7f215dbbd810>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215c78c410>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215c781650>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f215c7543d0>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215e219750>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215a857f10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f215e219350>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f215a858590>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6b94710>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6b99e90>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f2158656950>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba2a10>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba9050>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6ba2d10>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6bb3b50>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21586d8c10>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bbdb90>,\n",
       " <tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bb0710>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6b49410>]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "435cc4e8",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "layer_model.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48990087",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.layers.convolutional.Conv2D at 0x7f21d6bb0710>,\n",
       " <tensorflow.python.keras.layers.pooling.MaxPooling2D at 0x7f21d6b49410>]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer_model.layers[-2:] # Set the two last layers as trainable (including the last Conv2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d728e14c",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "for layer in layer_model.layers[-2:]:\n",
    "    layer.trainable = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c5fa0b5",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainable_layer_count = 0\n",
    "\n",
    "for i in range(len(layer_model.layers)):\n",
    "    if layer_model.layers[i].trainable:\n",
    "        trainable_layer_count += 1\n",
    "        \n",
    "trainable_layer_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69922475",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Data augmentation layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4ccb8822",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "data_augmentation_layers = models.Sequential([\n",
    "    RandomFlip(\"horizontal\", input_shape=(224, 224,3)),\n",
    "    RandomRotation(0.3),\n",
    "    RandomZoom(0.3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08559d31",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e50f6d8e",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session() # Clear the layers name (in case you run multiple time the cell)\n",
    "\n",
    "inputs = Input(shape=(224, 224, 3))\n",
    "\n",
    "x = data_augmentation_layers(inputs) # Are not applied to validation and test dataset (made inactive, tensorflow handle it)\n",
    "x = applications.vgg16.preprocess_input(x) # Does the rescaling\n",
    "x = layer_model(x) \n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.Dropout(0.5)(x) # Dropout to prevent overfitting\n",
    "\n",
    "outputs = layers.Dense(NUM_CLASSES, activation='softmax', name='classification_layer')(x)\n",
    "\n",
    "model = Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac94ac56",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Callbacks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6b1d8",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "85f06977",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "es = EarlyStopping(monitor='val_loss', patience=20, mode='min', restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ea1665",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27c6dc98",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# ! Do not use ReduceLROnPlateau if your optimizer alread handle learning rate modification !\n",
    "rlrp = ReduceLROnPlateau(monitor='val_loss', factor=0.4, patience=3, min_lr=1e-8)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd5ae4b9",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "#### ModelCheckpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "76ee4656",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "# You can add it to the callbacks if you want to save checkpoints\n",
    "checkpoint_dir = f\"{MAIN_PATH}logs/{USERNAME}/{MODEL}/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + f\"-unfreeze_{trainable_layer_count}\"\n",
    "mcp = ModelCheckpoint(\n",
    "    filepath=checkpoint_dir,\n",
    "    save_weights_only=True,\n",
    "    monitor='val_accuracy',\n",
    "    mode='max',\n",
    "    save_freq=10,\n",
    "    save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "60b54c14",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "recorded_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") \n",
    "log_dir = f\"{MAIN_PATH}logs/{USERNAME}/{MODEL}/\" + \\\n",
    "    recorded_time + \\\n",
    "    f\"-images_{total_images_count}\" + \\\n",
    "    f\"-unfreeze_{trainable_layer_count}\" + \\\n",
    "    f\"-batch_{BATCH_SIZE}\"\n",
    "\n",
    "tsboard = TensorBoard(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc2830b",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7888a07b",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=Adamax(learning_rate=0.001), \n",
    "                    loss='categorical_crossentropy',\n",
    "                    metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb33c553",
   "metadata": {
    "Collapsed": "false"
   },
   "source": [
    "### Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06549ef1",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "72/72 [==============================] - 86s 616ms/step - loss: 4.3145 - accuracy: 0.3708 - val_loss: 1.9313 - val_accuracy: 0.5347\n",
      "Epoch 2/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 2.3179 - accuracy: 0.4596 - val_loss: 1.6383 - val_accuracy: 0.5391\n",
      "Epoch 3/1000\n",
      "72/72 [==============================] - 43s 592ms/step - loss: 1.8028 - accuracy: 0.4869 - val_loss: 1.3055 - val_accuracy: 0.5582\n",
      "Epoch 4/1000\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 1.5749 - accuracy: 0.4976 - val_loss: 1.2923 - val_accuracy: 0.5747\n",
      "Epoch 5/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.5137 - accuracy: 0.4976 - val_loss: 1.2581 - val_accuracy: 0.5530\n",
      "Epoch 6/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.5201 - accuracy: 0.5005 - val_loss: 1.2272 - val_accuracy: 0.5521\n",
      "Epoch 7/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.4973 - accuracy: 0.4916 - val_loss: 1.2226 - val_accuracy: 0.5608\n",
      "Epoch 8/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.3835 - accuracy: 0.5146 - val_loss: 1.1409 - val_accuracy: 0.5781\n",
      "Epoch 9/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.3000 - accuracy: 0.5308 - val_loss: 1.1093 - val_accuracy: 0.5903\n",
      "Epoch 10/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2948 - accuracy: 0.5294 - val_loss: 1.1020 - val_accuracy: 0.5911\n",
      "Epoch 11/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2819 - accuracy: 0.5328 - val_loss: 1.1210 - val_accuracy: 0.5877\n",
      "Epoch 12/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2804 - accuracy: 0.5329 - val_loss: 1.1332 - val_accuracy: 0.5703\n",
      "Epoch 13/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2907 - accuracy: 0.5301 - val_loss: 1.0814 - val_accuracy: 0.5998\n",
      "Epoch 14/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2993 - accuracy: 0.5244 - val_loss: 1.1229 - val_accuracy: 0.5738\n",
      "Epoch 15/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2840 - accuracy: 0.5311 - val_loss: 1.1167 - val_accuracy: 0.5790\n",
      "Epoch 16/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2958 - accuracy: 0.5286 - val_loss: 1.0670 - val_accuracy: 0.5929\n",
      "Epoch 17/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2263 - accuracy: 0.5393 - val_loss: 1.0296 - val_accuracy: 0.6111\n",
      "Epoch 18/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2240 - accuracy: 0.5492 - val_loss: 1.0436 - val_accuracy: 0.6042\n",
      "Epoch 19/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.2167 - accuracy: 0.5481 - val_loss: 1.0475 - val_accuracy: 0.5964\n",
      "Epoch 20/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.2146 - accuracy: 0.5485 - val_loss: 1.0570 - val_accuracy: 0.5946\n",
      "Epoch 21/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.2091 - accuracy: 0.5495 - val_loss: 1.0413 - val_accuracy: 0.6007\n",
      "Epoch 22/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1984 - accuracy: 0.5524 - val_loss: 1.0419 - val_accuracy: 0.6024\n",
      "Epoch 23/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1886 - accuracy: 0.5581 - val_loss: 1.0399 - val_accuracy: 0.5998\n",
      "Epoch 24/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1807 - accuracy: 0.5587 - val_loss: 1.0337 - val_accuracy: 0.5990\n",
      "Epoch 25/1000\n",
      "72/72 [==============================] - 43s 595ms/step - loss: 1.1860 - accuracy: 0.5637 - val_loss: 1.0315 - val_accuracy: 0.6085\n",
      "Epoch 26/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1866 - accuracy: 0.5627 - val_loss: 1.0394 - val_accuracy: 0.5946\n",
      "Epoch 27/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1837 - accuracy: 0.5574 - val_loss: 1.0360 - val_accuracy: 0.5981\n",
      "Epoch 28/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1738 - accuracy: 0.5636 - val_loss: 1.0342 - val_accuracy: 0.6042\n",
      "Epoch 29/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1835 - accuracy: 0.5594 - val_loss: 1.0332 - val_accuracy: 0.6068\n",
      "Epoch 30/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1689 - accuracy: 0.5716 - val_loss: 1.0325 - val_accuracy: 0.6076\n",
      "Epoch 31/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1867 - accuracy: 0.5543 - val_loss: 1.0331 - val_accuracy: 0.6085\n",
      "Epoch 32/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1577 - accuracy: 0.5615 - val_loss: 1.0343 - val_accuracy: 0.6050\n",
      "Epoch 33/1000\n",
      "72/72 [==============================] - 43s 596ms/step - loss: 1.1823 - accuracy: 0.5548 - val_loss: 1.0342 - val_accuracy: 0.6050\n",
      "Epoch 34/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1690 - accuracy: 0.5673 - val_loss: 1.0333 - val_accuracy: 0.6042\n",
      "Epoch 35/1000\n",
      "72/72 [==============================] - 43s 593ms/step - loss: 1.1689 - accuracy: 0.5650 - val_loss: 1.0327 - val_accuracy: 0.6033\n",
      "Epoch 36/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1840 - accuracy: 0.5540 - val_loss: 1.0331 - val_accuracy: 0.6024\n",
      "Epoch 37/1000\n",
      "72/72 [==============================] - 43s 594ms/step - loss: 1.1791 - accuracy: 0.5604 - val_loss: 1.0330 - val_accuracy: 0.6033\n",
      "INFO:tensorflow:Assets written to: /home/jupyter/models/pablo/VGG16/20210830-122006-images_11520-unfreeze_2-batch_128/assets\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_ds, \n",
    "    epochs=EPOCHS, \n",
    "    validation_data=val_ds, \n",
    "    callbacks=[es, tsboard], \n",
    "    use_multiprocessing=True)\n",
    "\n",
    "model.save(f\"{MAIN_PATH}models/{USERNAME}/{MODEL}/\" + \\\n",
    "    recorded_time + \\\n",
    "    f\"-images_{total_images_count}\" + \\\n",
    "    f\"-unfreeze_{trainable_layer_count}\" + \\\n",
    "    f\"-batch_{BATCH_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d85eab",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs_range = history.epoch\n",
    "\n",
    "fig = plt.figure(figsize=(16, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, acc, label='Training Accuracy')\n",
    "plt.plot(epochs_range, val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, loss, label='Training Loss')\n",
    "plt.plot(epochs_range, val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "fig.savefig(f\"/home/jupyter/figures/{USERNAME}/{MODEL}/{recorded_time}-images_{total_images_count}-unfreeze_{trainable_layer_count}-batch_{BATCH_SIZE}\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e143d62",
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_ds, callbacks=tsboard)"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "tf2-gpu.2-5.m76",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-5:m76"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
